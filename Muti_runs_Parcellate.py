"""
# @ creater by Qi Wang on 2022.7.28
# Description: Individualized calculation function. 
  The data in the matching library and the data to be matched are both called from this. 
  For specific function parameters, please refer to Muti_runs-Parcellate.py.
  Parameters:
    --subject: unique index of data/subject names.
    --Adjacent_lh,Adjacent_rh: Geodetic distance of neighboring vertices.
    --L_lh,L_rh: The initial spatial label is the Yeo graph for HCP data 
        and the matched library data result for the data to be matched.
    --output_path: Output path.
    --work_path: Work path.
    --ss: Different runs.
    --Sim_thr: Convergence limit. The larger, the stricter.
    --Assign_thr: The threshold that determines whether to allocate the vertex to the network 
        can eliminate the discrete points generated by noise.
    --num_Iter: Maximum iteration round. 
        Constrained by early termination, it is generally not achieved. Prevent a dead loop.
    --Single: Distinguish between multiple iterations and a matching process. 
            False is the calculation of multiple iterations until convergence.
# Usage:
    By using a unique index of data/subject names, as the matching database data contains multiple runs 
    of data, even if the data to be matched is single run data, a run name needs to be set.
"""



import scipy.io as scio
import numpy as np
import random
import csv
import os
import matplotlib.pyplot as plt
import pandas as pd
import math


# os.environ["OUTDATED_RAISE_EXCEPTION"] = "1"
os.environ["OUTDATED_IGNORE"] = "1"

def mean2(x):
    y = np.sum(x) / np.size(x)
    return y

def corr2(a,b):
    a = a - mean2(a)
    b = b - mean2(b)

    r = (a*b).sum() / math.sqrt((a*a).sum() * (b*b).sum())
    return r

def similarity(v_a,v_b):
    num=0
    for i in range(v_a.shape[0]):
        if v_a[i]==v_b[i]:
            num+=1
    return num/v_a.shape[0]

def Indivdual_parcellate(subject,Adjacent_lh,Adjacent_rh,L_lh,L_rh,output_path,work_path,ss=["R1_LR","R1_RL","R2_RL","R2_LR"],Sim_thr=0.999,Assign_thr=0.045,num_Iter=30,Single=False):
    
    save_path=output_path+subject+"/"
    length=len(ss)
    # Initial label
    save_flag=0
    Label_lh=L_lh.flatten()
    Label_rh=L_rh.flatten()
    n=Label_lh.shape[0] # n vertex
    
    # Template labels
    Temp_lh=np.array(list(Label_lh))
    Temp_rh=np.array(list(Label_rh))

    # ad_num=Adjacent_lh.shape[1]

    Time_lh={} # Each individual's time series only read once
    Time_rh={}
    Rel_lh={} # Each individual's reliability only read once
    Rel_rh={}
        
    sim_lh=0 # Left and right hemi similarities
    sim_rh=0
    num_lh,num_rh=0,0


    if os.path.exists(save_path):
        pass
    else:
        os.mkdir(save_path)    

    for num in range(num_Iter):
        
        # if num+1>=2:  
        # Use the previous run of labels
            # Label_lh=pd.readcsv(save_path+str(num)+'iter_label_lh.csv',header=None).values
            # Label_rh=pd.readcsv(save_path+str(num)+'iter_label_rh.csv',header=None).values
            # Label_lh=Res_Label_lh
            # Label_rh=Res_Label_rh
            # handle nan
            

        print("ID:",subject,";Parcellating: "," iter: ",num)
        Conf_Prob_lh=np.zeros((n,17)) # confidence probability
        Conf_Prob_rh=np.zeros((n,17))

        for run in range(length):
            
            if num+1==1:  # The first run 
                program_path=work_path+ss[run]+"/"+subject
                
                Time_lh[ss[run]]=pd.read_csv(program_path+"/surf_lh_time_pial.csv",header=None).values # 10242*1200 timecourse
                Time_rh[ss[run]]=pd.read_csv(program_path+"/surf_rh_time_pial.csv",header=None).values 

                Time_lh[ss[run]]=np.nan_to_num(Time_lh[ss[run]],nan=0)
                Time_rh[ss[run]]=np.nan_to_num(Time_rh[ss[run]],nan=0)
               
             
                # # time sample
                # sample_lh=[]
                # sample_rh=[]
                # for kk in range(0,1200,10):
                #     sample_lh.append(np.mean(Time_lh[ss[run]][:,kk:kk+14],1))
                #     sample_rh.append(np.mean(Time_rh[ss[run]][:,kk:kk+14],1))
            
                # Time_lh[ss[run]]=np.array(sample_lh).T
                # Time_rh[ss[run]]=np.array(sample_rh).T
          
             

                # Time_lh[ss[run]]=Time_lh[ss[run]][:,600:1200]
                # Time_rh[ss[run]]=Time_rh[ss[run]][:,600:1200]
                
                if Single==False:
                # MRR
                    Rel_lh[ss[run]]=pd.read_csv(program_path+"/reliability_pial_0.01_lh.csv",header=None).values # 10242*1 reliability
                    Rel_rh[ss[run]]=pd.read_csv(program_path+"/reliability_pial_0.01_rh.csv",header=None).values 
                else:
                    print("No MRR exists!")
                    Rel_lh[ss[run]]=np.ones(n)*0.535
                    Rel_rh[ss[run]]=np.ones(n)*0.535

            # Combine the left and right hemispheres
            Seed=[]
            
            for i in range(17): # 17 networks
                
                idx_lh=np.where(Label_lh==i+1)
                temp_lh=Time_lh[ss[run]][idx_lh]
                idx_rh=np.where(Label_rh==i+1)
                temp_rh=Time_rh[ss[run]][idx_rh]
              
                temp=np.vstack((temp_lh,temp_rh))
                Seed_Time=np.sum(temp,axis=0)/temp.shape[0] # Average time series in each network
                Seed.append(Seed_Time)
                
            Seed=np.array(Seed)
    
            # Left
            Data_lh=np.vstack((Seed,Time_lh[ss[run]]))
            rtemp_lh=np.corrcoef(np.array(Data_lh))
            rValue_lh=rtemp_lh[:Seed.shape[0],-n:].T
            rValue_lh=np.array(rValue_lh)
            # Right
            Data_rh=np.vstack((Seed,Time_rh[ss[run]]))
            rtemp_rh=np.corrcoef(np.array(Data_rh))
            rValue_rh=rtemp_rh[:Seed.shape[0],-n:].T
            rValue_rh=np.array(rValue_rh)
            
            # Combine muti-runs
            for k in range(n):
                rValue_lh[k]=rValue_lh[k]*Rel_lh[ss[run]][k]
                rValue_rh[k]=rValue_rh[k]*Rel_rh[ss[run]][k]
                
            Conf_Prob_lh+=rValue_lh
            Conf_Prob_rh+=rValue_rh  
                
        if sim_lh<Sim_thr or num<length :
            Conf_Prob_res_lh=Conf_Prob_lh/length
            num_lh+=1

        if sim_rh<Sim_thr or num<length:
            Conf_Prob_res_rh=Conf_Prob_rh/length
            num_rh+=1

       
        Conf_temp_lh=np.array(list(Conf_Prob_res_lh))
        Conf_temp_rh=np.array(list(Conf_Prob_res_rh))
        
        # Save confidence probability
        # np.savetxt(save_path+str(num+1)+'iter_zValue_lh.csv',Conf_Prob_lh,delimiter=',',fmt='%f')
        # np.savetxt(save_path+str(num+1)+'iter_zValue_rh.csv',Conf_Prob_rh,delimiter=',',fmt='%f')

        for j in range(n):
            # Smooth
           
            Conf_Prob_res_lh[j]=0*Conf_temp_lh[j]+1*np.mean(Conf_temp_lh[Adjacent_lh[j]-1],0)
            Conf_Prob_res_rh[j]=0*Conf_temp_rh[j]+1*np.mean(Conf_temp_rh[Adjacent_rh[j]-1],0)
            if Temp_lh[j]==0:
                Label_lh[j]=0
                continue
            # Label_lh[j]=np.argmax(Conf_Prob_lh[j])+1
            # Label_rh[j]=np.argmax(Conf_Prob_rh[j])+1
    
            # Left
            if sim_lh<Sim_thr or num<length:
            
                Conf_Prob_res_lh[j][Temp_lh[j]-1]+=0.15/(num_lh)
           
                if  (abs(np.max(Conf_Prob_res_lh[j])-np.sort(Conf_Prob_res_lh[j])[-2]) <Assign_thr) and num>0:
                
                    # random
                    # Label_lh[j]=Temp_lh[j]
                    # if num+1==1:
                    #     Label_lh[j]=Temp_lh[j]
                    #     # Label_lh[j]=random.randint(1,17)
                    continue          
                else:     
                    Label_lh[j]=np.argmax(Conf_Prob_res_lh[j])+1
                   

                # Right
            if sim_rh<Sim_thr or num<length:
              
                Conf_Prob_res_rh[j][Temp_rh[j]-1]+= 0.15/(num_rh)
                if  (abs(np.max(Conf_Prob_res_rh[j])-np.sort(Conf_Prob_res_rh[j])[-2])<Assign_thr) and num>0:
                    continue
                else:
                    Label_rh[j]=np.argmax(Conf_Prob_res_rh[j])+1  
        

        if num+1>=2: # Early termination
            sim_lh=similarity(Res_Label_lh,Label_lh)
            sim_rh=similarity(Res_Label_rh,Label_rh)
            
            print(sim_lh,sim_rh)

            if sim_lh>=Sim_thr and sim_rh>=Sim_thr and num>=length:
                save_flag=1
                # In order to draw and read .csv
                np.savetxt(save_path+ss[run]+'no25_535final_iter_label_0.045_pial_lh.csv',Label_lh,delimiter=',',fmt='%f')
                np.savetxt(save_path+ss[run]+'no25_535final_iter_label_0.045_pial_rh.csv',Label_rh,delimiter=',',fmt='%f')
                # np.savetxt(save_path+'final_iter_Prob_lh.csv',Conf_Prob_res_lh,delimiter=',',fmt='%f')
                # np.savetxt(save_path+'final_iter_Prob_rh.csv',Conf_Prob_res_rh,delimiter=',',fmt='%f')
                print(subject, " Iteration stops!",num_lh,num_rh)
                break
            
        Res_Label_lh=np.array(list(Label_lh)) # upgrade
        Res_Label_rh=np.array(list(Label_rh))
       
    if save_flag==0 and num+1>=1:
        np.savetxt(save_path+ss[run]+'no25_535final_iter_label_0.045_pial_lh.csv',Label_lh,delimiter=',',fmt='%f')
        np.savetxt(save_path+ss[run]+'no25_535final_iter_label_0.045_pial_rh.csv',Label_rh,delimiter=',',fmt='%f')
        print(subject, "Exceed the num_iter!")
    return  